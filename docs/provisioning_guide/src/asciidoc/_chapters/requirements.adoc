////
/**
* @@@ START COPYRIGHT @@@
*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*   http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing,
* software distributed under the License is distributed on an
* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
* KIND, either express or implied.  See the License for the
* specific language governing permissions and limitations
* under the License.
*
* @@@ END COPYRIGHT @@@
*/
////

[[requirements]]
= Requirements

{project-name} requires an x86 version of Linux. 

The current release of {project-name} has been tested with:

* 64-bit Red Hat Enterprise Linux (RHEL) or CentOS 6.5 - 6.8
* Cloudera CDH 5.4 - 5.7
* Hortonworks HDP 2.3 - 2.4

Other OS releases may work, too. The {project-name} project is currently working on better support for more distribution and non-distribution versions of Hadoop.


[[requirements-general-cluster-and-os-requirements-and-recommendations]]
== General Cluster and OS Requirements and Recommendations

64-bit x86 instruction set running a Linux distribution is required. Further, {project-name} assumes an environment 
based on the requirements of the tested Hadoop distributions/services. 

[[requirements-cluster-requirements-and-recommendations]]
=== Hardware Requirements and Recommendations

[[requirements-single-node-cluster]]
==== Single-Node Cluster
It is possible to run {project-name} on a single-node sandbox environment. Typically, any sandbox running a Hadoop distribution 
can be used. A typical single-node configuration uses 4-8 cores with 16 GB of memory, and  20 GB free disk space.

[[requirements-multi-node-cluster]]
==== Multi-Node Cluster 
For multi-node end-user clusters, your typical HBase environment should suffice for {project-name}.
Typically, memory configuration range between 64-128 GB per node with minimum requirement of 16 GB. 
The cluster size can span from 1 to _n_ nodes; a minimum of two nodes is recommended. 
A minimum of two cores is required regardless of whether you're deploying {project-name} on a bare-metal or virtual environment. 

<<<
Recommended configurations:

[cols="40%h,60%a",options="header"]
|===
| Attribute | Guidance
| Processors per Node |
&#8226; Small: 2 cores +
&#8226; Medium: 4 cores +
&#8226; Large: 8+ cores
| Memory per Node |
&#8226; Small: 16 GB +
&#8226; Medium: 64 GB +
&#8226; Large: 128 GB
| Concurrency:Nodes |
&#8226; Two Small Nodes: Four concurrent queries +
&#8226; Two Medium Nodes: 64 concurrent queries +
&#8226; Two Large Nodes: 256 concurrent queries
|===

[[requirements-os-requirements-and-recommendations]]
=== OS Requirements and Recommendations

Please verify these requirements on each node you will install {project-name} on:

[cols="20%a,40%a,40%a",options="header"]
|===
| Function | Requirement                                                                                  | Verification Guidance
| Linux    | 64-bit version of Red Hat(RHEL) or CentOS 6.5 -6.8                                           | `cat /etc/redhat-release`
| sshd     | The `ssh` daemon is running on each node in the cluster.                                     |
&#8226; `ps aux  \| grep sshd` +
&#8226; `sudo netstat -plant \| grep :22`
| ntpd     | The `ntp` daemon is running and synchronizing time on each node in the cluster.              |
&#8226; `ps aux \| grep ntp` +
&#8226; `ntpq -p`
| FQDN    | 
&#8226; `/etc/hosts` is set up for fully-qualified node names (FQDN). +
&#8226; `/etc/resolv.conf` is configured to use a name server. |  
&#8226; `hostname --fqdn` shows the fully-qualified node name, if any. +
&#8226; The fully-qualified node name is part of the `/etc/hosts` file. +
&#8226; `host -T <FQDN>` (responds if using a DNS server, times out otherwise) +
&#8226; Simply ssh among nodes using `ssh <FQDN>`. 
| Port Availability | The Linux Kernel Firewall (`iptables`) has either been disabled or <<ip-ports,ports required by {project-name}>> have been opened. |
&#8226; `lsmod \| grep ip_tables` checks whether `iptables` is loaded. If not, no further checking is needed. +
&#8226; `sudo iptables -nL \| grep <port>` checks the configuration of a port. An empty response indicates no rule for the port, which often means 
the port is *not* open.
| passwordless ssh | The user name used to provision {project-name} must have passwordless ssh access to all nodes. | ssh to the nodes, ensure that no password prompt appears.
| sudo privileges  | The user name used to provision {project-name} must sudo access to a number of root functions . | `sudo echo "test"` on each node.
| bash     | Available for shell-script execution.                                                        | `bash --version`
| java     | Available to run the {project-name} software. Same version as HBase is using.                | `java --version`
| perl     | Available for script execution.                                                              | `perl --version`
| python   | Available for script execution.                                                              | `python --version`
| yum      | Available for installs, updates, and removal of software packages.                           | `yum --version`
| rpm      | Available for installs, updates, and removal of software packages.                           | `rpm --version`
| scp      | Available to copy files among nodes in the cluster.                                          | `scp --help`
|===


[[requirements-ip-ports]]
=== IP Ports
The following table lists the default ports used by the different {project-name} components plus the configuration file and configuration attribute associated with each port setting.

[cols="10%h,20%l,20%l,10%,5%,10%,25%",options="header"]
|===
| Default Port | Configuration File | Configuration Entry             | Required | Range | Protocol | Comment 
| 4200         | rest-site.xml      | trafodion.rest.port             | Yes      | 1     | REST     | {project-name} REST Server.
| 4201         | rest-site.xml      | trafodion.rest.https.port       | Yes      | 1     | HTTPS    | {project-name} REST Server (HTTPS).
| 23400        | dcs-site.xml       | dcs.master.port                 | Yes      | _n_   | binary   | Start of {project-name} DCS port range. (37800 for {project-name} 1.1)
| 24400        | dcs-site.xml       | dcs.master.info.port            | Yes      | 1     | HTTP     | DCS master web GUI. (40010 for {project-name} 1.1)
| 24410        | dcs-site.xml       | dcs.server.info.port            | Yes      | _n_   | HTTP     | Start of range for DCS server web GUIs. (40030 for {project-name} 1.1)
| 50030        | mapred-site.xml    | mapred.job.tracker.http.address | No       | 1     | HTTP     | MapReduce Job Tracker web GUI.
| 50070        | hdfs-site.xml      | dfs.http.address                | No       | 1     | HTTP     | HDFS Name Node web GUI.
| 50075        | hdfs-site.xml      | dfs.datanode.http.address       | No       | 1     | HTTP     | HDFS Data Node web GUI.
| 50090        | hdfs-site.xml      | dfs.secondary.http.address      | No       | 1     | HTTP     | HDFS Secondary Name Node web GUI.
| 60010        | hbase-site.xml     | hbase.master.info.port          | No       | 1     | HTTP     | HBase Master web GUI.
| 60030        | hbase-site.xml     | hbase.regionserver.info.port    | No       | 1     | HTTP     | HBase Region Server web GUI.
|===

There are two port ranges used by {project-name}.

* 23400 is a range, to allow multiple mxosrvr processes on each node. Allow a range of a few ports,
enough to cover all the servers per node that are listed in the "servers" file in the DCS configuration directory.
* 24410 is a range as well, enough to cover the DCS servers per node, usually 1 or 2.

On top of the ports identified above, you also need the ports required by your Hadoop distribution. For example:

* http://www.cloudera.com/content/www/en-us/documentation/enterprise/latest/topics/cdh_ig_ports_cdh5.html[_Cloudera Ports_]
* http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0-Win/bk_HDP_Install_Win/content/ref-79239257-778e-42a9-9059-d982d0c08885.1.html[_Hortonworks Ports_]

If you have Kerberos or LDAP enabled, then ports required by these products need to be opened as well.

Although not all the ports will be used on every node of the cluster, you need to open most of them for all the nodes in the cluster that
have {project-name}, HBase, or HDFS servers on them.

[[requirements-prerequisite-software]]
== Prerequisite Software

[[requirements-hadoop-software]]
=== Hadoop Software

{project-name} runs as an add-on service on Hadoop distributions. The following Hadoop services and their dependencies must be installed and running 
on the cluster where you intend to install {project-name}:

* Hadoop Distributed File System (HDFS)
* ZooKeeper
* HBase
* Hive
* Apache Ambari (Hortonworks) or Cloudera Manager (Cloudera) with associated embedded databases.

[[requirements-software-packages]]
=== Software Packages

In addition to the software packages required to run different Hadoop services listed above (for example, `Java`), 
{project-name} requires supplementary software to be installed on the cluster before it is installed. These are Linux
tools that are not typically packaged as part of the core Linux distribution.

NOTE: For RedHat/CentOS, the {project-name} Installer automatically attempts get a subset of these packages over the Internet.
If the cluster's access to the Internet is disabled, then you need to manually download the packages and make them available
for installation.

[cols="20%,45%,35%l",options="header"]
|===
| Package              | Usage                                                                             | Installation
| EPEL                 | Add-on packages to completed the Linux distribution.                              | Download
http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch[Fedora RPM]
| pdsh                 | Parallelize shell commands during install and {project-name} runtime utilities.        | yum install pdsh
| sqlite               | Internal configuration information managed by the {project-name} Foundation component. | yum install sqlite
| expect               | Not used?                                                                         | yum install expect
| perl-DBD-SQLite      | Allows Perl scripts to connect to SQLite.                                         | yum install perl-DBD-SQLite
| perl-Params-Validate | Validates method/function parameters in Perl scripts.                              | yum install perl-Params-Validate
| perl-Time-HiRes      | High resolution alarm, sleep, gettimeofday, interval timers in Perl scripts.      | yum install perl-Time-HiRes
| protobuf             | Data serialization.                                                               | yum install protobuf
| xerces-c             | C++ XML parsing.                                                                  | yum install xerces-c
| gzip                 | Data compress/decompress.                                                         | yum install gzip
| apr-devel            | Support files used to build applications using the APR library.                   | yum install apr-devel
| apr-util-devel       | Support files used to build applications using the APR utility library.           | yum install apr-util-devel
|===

The {project-name} Installer requires both Internet access/Offline mode to install the required software packages.
Specify `db_install.py --offline` to use the offline install feature. Before that, you need to prepare a local repository
folder for all the above dependencies.
To create a local repository, be sure the `createrepo` package is installed, then run createrepo command in your rpm folder.
```
$ createrepo -d .
```

[[requirements-trafodion-user-ids-and-their-privileges]]
== {project-name} User IDs and Their Privileges

[[requirements-trafodion-runtime-user]]
=== {project-name} Runtime User

The `trafodion:trafodion` user ID is created as part of the installation process. The default password is: `traf123`.

{project-name} requires that either HDFS ACL support or Kerberos is enabled. The {project-name} Installer will enable HDFS ACL and Kerberos support. Refer to <<enable-security-kerberos,Kerberos>> for more information about the requirements and usage of Kerberos in Trafodion.
Refer to https://hbase.apache.org/book.html#security[Apache HBase(TM) Reference Guide] for security in HBase. 

NOTE: Do *not* create the `trafodion:trafodion` user ID in advance. The {project-name} Installer uses the presence of this user ID to determine
whether you're doing an installation or upgrade.

[[requirements-trafodion-provisioning-user]]
=== {project-name} Provisioning User

Typically, the {project-name} Installer is used for {project-name} installations. It requires access to the user IDs documented below.

[[requirements-linux-installation-user]]
==== Linux Installation User
The user ID that performs the {project-name} installation steps. Typically, this User ID runs the {project-name} Installer.

*Requirements*:

* User name or group cannot be `trafodion`.
* Passwordless ssh access to all nodes in the cluster.
* Internet access to download software packages.
* `requiretty` must be disabled in `/etc/sudoers`.
* `sudo`^1^ access to:
** Download and install software packages.
** Modify `/etc/sudoers.d` (allow the `trafodion` user to modify floating IP: `ip` and `arping`).
** Create the `trafodion` user ID and group.
** Install {project-name} software into the HBase environment.
** Run Java version command on each node in the cluster.
** Run Hadoop version command on each node in the cluster.
** Run HBase version command on each node in the cluster.
** Create directories and files in `/etc/trafodion`:
** Invoke `su` to execute commands as other users; for example, `trafodion`.
** Edit `sysctl.conf` and activate changes using `sysctl -p`:
*** Modify kernel limits.
*** Reserve IP ports.

^1^ `sudo` is *required* in the current release of {project-name}. This restriction may be relaxed in later releases.
Alternative mechanisms for privileged access (such as `sudo` alternative commands) are not supported.

[[requirements-distribution-manager-user]]
==== Distribution Manager User
A user ID that can change the configuration using Apache Ambari or Cloudera Manager. The {project-name} Installer makes REST
request to perform configuration and control functions to the distribution manager using this user ID.

*Requirements*:

* Administrator user name and password.
* URL to Distribution Manager's REST API.

[[requirements-hdfs-administrator-user]]
==== HDFS Administrator User
The HDFS super user. Required to create directories and change security settings, as needed.
The {project-name} Installer uses `su` to run commands under this user ID.

*Requirements*:

* HDFS Administrator user name.
* Write access to home directory on the node where the Distribution Manager is running.
* For Kerberos enabled installations, location of the keytab for the HDFS service principal.

[[requirements-hbase-administrator-user]]
==== HBase Administrator User
The HBase super user. Required to change directory ownership in HDFS. For Kerberos enabled installations, the HBase super user is needed to grant the `trafodion` user create, read, write, and execute privileges.

*Requirements*:

* HBase Administrator user name and group.
* Read access to `hbase-site.xml`.
* For Kerberos enabled installations, location of the keytab for the HBase service principal.

[[requirements-kerberos-administrator-user]]
==== Kerberos Administrator User
The Kerberos adminstrator. Required to create Trafodion principals and keytabs on a cluster where Kerberos is enabled. 

*Requirements*:

* Kerberos Administrator admin name including the realm.
* Kerberos Administrator password

== Recommended Configuration Changes
The following configuration changes are recommended but not required.

NOTE: The {project-name} Installer does *not* make these changes.

[[requirements-recommended-security-changes]]
=== Recommended Security Changes

The `trafodion` user ID should not be given other `sudo` privileges than what's specified in this manual. Also, we
recommend that this user ID is locked (`sudo passwd -l trafodion`) once the installation/upgrade activity has been completed.
Users that need issue commands as the `trafodion` ID should do so using `sudo`; for example, `sudo -u trafodion -i`.


[[requirements-recommended-hdfs-configuration-changes]]
=== Recommended HDFS Configuration Changes

These settings are configured in the `hadoop-env.sh` file.

[cols="40%l,20%,40%a",options="header"]
|===
| Property                          | Recommended Setting | Guidance
| DataNode Java Heap Size           | 2 GB                | Use this setting for a large configuration.
| NameNode Java Heap Size           | 2 GB                | Use this setting for a large configuration.
| Secondary NameNode Java Heap Size | 2 GB                | Use this setting for a large configuration.
|===

[[requirements-recommended-hbase-configuration-changes]]
=== Recommended HBase Configuration Changes

[cols="30%l,20%,50%a",options="header"]
|===
| Configuration Property | Recommended Setting | Guidance
| hbase.rpc.timeout | 10 minutes | This setting depends on the tables' size. Sixty (60) seconds is the default. 
Increase this value for big tables. Make it the same value as `hbase.client.scanner.timeout.period`. We have found 
that increasing the setting to six-hundred (600) seconds will prevent many of the timeout-related errors we encountered, 
such as `OutOfOrderNextException` errors.
| hbase.client.scanner.timeout.period | 10 minutes | Similar to the `hbase.rpc.timeout` setting. Sixty (60) seconds is the 
default. Depending on the size of a user table, we have experienced timeout failures on count(*) and update statistics commands 
from this setting. The underlying issue is the length of the execution of the coprocessor within HBase.
 +
NOTE: HBase uses the smaller of `hbase.rpc.timeout` and `hbase.client.scanner.timeout.period` to calculate the scanner timeout. 
| hbase.snapshot.master.timeoutMillis and hbase.snapshot.region.timeout | 10 minutes | HBase's default setting is 60000 milliseconds. 
If you experience timeout issues with HBase snapshots when you use the {project-name} Bulk Loader or other statements, 
you can set the value for these two HBase properties to 10 minutes (600,000 milliseconds).
| hbase.hregion.max.filesize | 107374182400 bytes | HBase's default setting is 10737418240 (10 GB). We have increased the setting to 
107374182400 (100 GB), which reduces the number of HStoreFiles per table and appears to reduce disruptions to active transactions from 
region splitting.
| hbase.hstore.blockingStoreFiles | 10 | http://gbif.blogspot.com/2012/07/optimizing-writes-in-hbase.html
| hbase.regionserver.handler.count | <num> | This setting should match the number of concurrent sessions (mxosrvr). The default is 10.
|===

