////
/**
* @@@ START COPYRIGHT @@@
*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*   http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing,
* software distributed under the License is distributed on an
* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
* KIND, either express or implied.  See the License for the
* specific language governing permissions and limitations
* under the License.
*
* @@@ END COPYRIGHT @@@
*/
////

[[troubleshoot]]
= Troubleshoot
== Improving Throughput 

=== Tuplelists or Rowsets

When Tuplelists or Rowsets are used as the data source, performance typically increases with the
number of rows in the Tuplelist or Rowset. Performance peaks at some value for the number of rows
and remain more or less steady after that. This peak value depends on row size.
Typically a value in the range of 100 to few thousand is reasonable.

=== Native HBase Tables

When native HBase tables are used as the data source, it is important to override the default value
for the attribute HBASE_MAX_COLUMN_VALUE_LENGTH (columnwise mode) or HBASE_MAX_COLUMN_INFO_LENGTH (rowwise mode)
and set the value to the maximum for the table being used as the source.
The default values may be too large.

=== Hive Tables

When Hive tables are used as the data source, it is important to override the default value for the
attribute HIVE_MAX_STRING_LENGTH when the Hive source table has columns of type string. Please set the
value to the length of the longest string in the Hive table.

To determine that length, run this query from a Hive shell:

```
SELECT MAX(LENGTH(<col-name>)) FROM <hive-tab-name>;
```

If the query returns a value less than the current HIVE_MAX_STRING_LENGTH, then you need to
increase that value and retry. If the query  returns a value that is far less than the current
HIVE_MAX_STRING_LENGTH, then you can achieve better performance by reducing the value. 
An approximate value can be used, too. The {project-name} default of 32000 may be too generous in some cases.

== Checking Plan Quality

It is good practice to check the quality of the plan generated by the SQL compiler before executing a
data loading statement that may take a long time to complete. 

* For INSERT and UPSERT USING LOAD statements, use the EXPLAIN statement, which is described in the 
{docs-url}/sql_reference/index.html#explain_statement[{project-name} SQL Reference Manual].
* For the LOAD statement, which is implemented as a utility operator (that is, a collection of secondary SQL statements),
use the following SQL statements to see the plan that it uses to add data to the target table:

```
CONTROL QUERY DEFAULT COMP_BOOL_226 'ON' ;
PREPARE s1 FROM LOAD TRANSFORM INTO <target-table> <select-query-used-as-source> ;
EXPLAIN OPTIONS 'f' s1 ;
```

A typical problem with the plan is that the scan is not parallel enough. For {project-name} tables, you can address this
issue with the default attribute, PARALLEL_NUM_ESPS.  Using this attribute, a {project-name} scan can be parallelized to
as many number of SALT partitions that are defined for the table. For Hive source tables, the default attributes, 
HIVE_NUM_ESPS_PER_DATANODE and HIVE_MIN_BYTES_PER_ESP_PARTITION, can be used to adjust the degree of parallelism.

== UPDATE STATISTICS Times Out During Sampling

Sampling in update statistics is implemented using the HBase Random RowFilter. For very large tables with several
billion rows, the sampling ratio required to get a sample of one million rows is very small. This can result in
HBase client connection timeout errors since there may be no row returned by a RegionServer for an extended period of time.

You can avoid this problem by:

* Choosing a sampling percentage higher than the default setting of 1 million rows for large tables.
+
For example, suppose table T has one billion rows. Use the following UPDATE STATISTICS statement to sample a million rows,
or approximately one-tenth of one percent of the total rows:
+
```
UPDATE STATISTICS FOR TABLE t ON EVERY COLUMN SAMPLE;
```
+
To sample one percent of the rows, regardless of the table size, you must explicitly state the sampling rate as follows:
+
```
UPDATE STATISTICS FOR TABLE t ON EVERY COLUMN SAMPLE RANDOM 1 PERCENT;
```

* Setting `hbase.rpc.timeout` to a higher value than currently specified in the HBase settings.

== Index Creation Takes Too Long

When creating an index, all rows of the {project-name} table must be scanned and a subset of columns is returned to the client.
This can take a while to complete.  If there is a Hive table with the same data as the {project-name} table being scanned, then
you can specify the default attribute, USE_HIVE_SOURCE. This causes the Hive table to be used as the source creating the index. 

NOTE: The name of the Hive table must use the {project-name} table name as its prefix.
For example, if the {project-name} table is TRAFODION.SCH.DEMO, then the Hive 
table name can be DEMO_SRC. In this case, set the attribute as follows: +
 +
```
CONTROL QUERY DEFAULT USE_HIVE_SOURCE '_SRC' ;
CREATE INDEX demo_ix ON sch.demo(name) ;
```

[[large-deletes]]
== Large Deletes Take Too Long or Error Out

If a large number of rows is either updated or deleted in a single SQL statement, then it is likely that the
statement does not complete successfully.

Deleting or updating more than 10,000 rows with a single statement is not recommended. Instead, a large delete
or update should be broken up into multiple statements  each affecting less than 10,000*n rows, if possible.
`n` is number of nodes in the cluster.

== Large UPSERT USING LOAD On a Table With Index Errors Out

UPSERT USING LOAD automatically reverts to a transactional UPSERT when used on a table with an index. This causes {project-name}
to run into the limitation discusses in <<large-deletes,Large Deletes Take Too Long or Error Out>> above:
no more than 10,000*n rows (n = number of nodes) can be affected in a single statement.

*Workaround*: The UPSERT USING LOAD operation can be placed in a LOAD statement as shown below. The LOAD statement disables
indexes on the table before the UPSERT USING LOAD starts. Once the UPSERT USING LOAD  completes indexes are populated by
the LOAD statement. 

```
LOAD WITH UPSERT USING LOAD INTO trafodion.sch.demo SELECT * FROM hive.hive.demo;

Task: LOAD             Status: Started    Object: TRAFODION.SCH.DEMO
Task:  DISABLE INDEXE  Status: Started    Object: TRAFODION.SCH.DEMO
Task:  DISABLE INDEXE  Status: Ended      Object: TRAFODION.SCH.DEMO
Task:  UPSERT USING L  Status: Started    Object: TRAFODION.SCH.DEMO
       Rows Processed: 200000 
Task:  UPSERT USING L  Status: Ended      ET: 00:01:03.715
Task:  POPULATE INDEX  Status: Started    Object: TRAFODION.SCH.DEMO
Task:  POPULATE INDEX  Status: Ended      ET: 00:03:11.323
```






