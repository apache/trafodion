#!/bin/bash

# @@@ START COPYRIGHT @@@
#
# (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#
# @@@ END COPYRIGHT @@@

#
# This script will configure HBase with HBase-trx 
# and co-processors needed for Trafodion.  It uses
# Cloudera Manager's REST api to do this.
#
# NOTE: Only for Cloudera installations

TRAF_CONFIG=/etc/trafodion/trafodion_config
source $TRAF_CONFIG

#=====================================
# copy Trafodion trx jar to Cloudera's plugins directory on all nodes

cd $UNTAR_DIR

# determine java version for hbase 0.98 only java 1.7 is allowed
echo "***INFO: Detected JAVA version $JAVA_VERSION"
if [ "$JAVA_VERSION" == "1.6" ]; then
   echo "***ERROR: Java 1.6 not supported for Trafodion HBase 0.98"
   exit -1
elif [ "$JAVA_VERSION" == "1.7" ]; then
	hbase_trx_jar="hbase-trx-${TRAF_VERSION}.jar"
else
    echo "***ERROR: unable to determine Hadoop's java version"
    exit -1
fi

# The permissions the Trafodion build process creates on the hbase-trx jar
# files does not work well with the installation process so we change them
sudo chmod -R 777 $UNTAR_DIR/export/lib

if [ ! -f $UNTAR_DIR/export/lib/$hbase_trx_jar ]; then
    echo "***ERROR: unable to find $UNTAR_DIR/export/lib/$hbase_trx_jar"
    exit -1
fi

# if more than one node then copy to all nodes
echo "***INFO: copying $hbase_trx_jar to all nodes"
if [ $node_count -ne 1 ]; then
    pdsh $MY_NODES sudo rm -rf /usr/lib/hbase/lib/hbase-trx* 2>/dev/null
    pdsh $MY_NODES sudo rm -rf /usr/share/cmf/lib/plugins/hbase-trx* 2>/dev/null
    pdsh $MY_NODES sudo rm -rf /usr/lib/hbase/lib/trafodion* 2>/dev/null
    pdsh $MY_NODES sudo rm -rf /usr/share/cmf/lib/plugins/trafodion* 2>/dev/null
    pdsh $MY_NODES mkdir -p $PWD 2>/dev/null
    cp $UNTAR_DIR/export/lib/$hbase_trx_jar $LOCAL_WORKDIR
    pdcp $MY_NODES $LOCAL_WORKDIR/$hbase_trx_jar $LOCAL_WORKDIR
    pdsh $MY_NODES sudo cp $LOCAL_WORKDIR/$hbase_trx_jar /usr/lib/hbase/lib
    pdsh $MY_NODES sudo chmod 644 /usr/lib/hbase/lib/$hbase_trx_jar
    pdsh $MY_NODES rm $LOCAL_WORKDIR/$hbase_trx_jar 2>/dev/null
else
    sudo rm -rf /usr/lib/hbase/lib/hbase-trx* 2>/dev/null
    sudo rm -rf /usr/share/cmf/lib/plugins/hbase-trx* 2>/dev/null
    sudo rm -rf /usr/lib/hbase/lib/trafodion* 2>/dev/null
    sudo rm -rf /usr/share/cmf/lib/plugins/trafodion* 2>/dev/null
    sudo cp $UNTAR_DIR/export/lib/$hbase_trx_jar /usr/lib/hbase/lib
    sudo chmod 644 /usr/lib/hbase/lib/$hbase_trx_jar
fi

#=====================================
# create new directories for bulkload and lobs if not already there
rm $LOCAL_WORKDIR/traf_temp_output 2>/dev/null

isClouderaRunning=$(sudo /etc/init.d/cloudera-scm-server status)

clouderaUp=$(echo $isClouderaRunning | grep "is running" | wc -l)

if [[ "$clouderaUp" -eq "0" ]]; then
   echo "***INFO: Cloudera Manager is down..."
   echo "***INFO: Restarting...."
   sudo /etc/init.d/cloudera-scm-server restart
fi

sudo su $HDFS_USER --command "hadoop fs -mkdir /hbase-staging" 2> $LOCAL_WORKDIR/traf_temp_output
if [ $? != 0 ]; then
   # ok if directory already exists
   dir_exists=$(grep "File exists" $LOCAL_WORKDIR/traf_temp_output | wc -l)
   if [ $dir_exists -eq 0 ]; then
      echo "***ERROR: 'hadoop fs -mkdir /hbase-staging' command failed"
      echo "***ERROR: $(cat $LOCAL_WORKDIR/traf_temp_output)"
      exit -1
   fi
fi
sudo su $HDFS_USER --command "hadoop fs -chown -R $HBASE_USER:$HBASE_GROUP /hbase-staging"
sudo su $HDFS_USER --command "hadoop fs -mkdir /bulkload" 2> $LOCAL_WORKDIR/traf_temp_output
if [ $? != 0 ]; then
   # ok if directory already exists
   dir_exists=$(grep "File exists" $LOCAL_WORKDIR/traf_temp_output | wc -l)
   if [ $dir_exists -eq 0 ]; then
      echo "***ERROR: 'hadoop fs -mkdir /bulkload' command failed"
      echo "***ERROR: $(cat $LOCAL_WORKDIR/traf_temp_output)"
      exit -1
   fi
fi
sudo su $HDFS_USER --command "hadoop fs -chown -R $TRAF_USER:trafodion /bulkload"

# Create lobs directory
sudo su hdfs --command "hadoop fs -mkdir /lobs" 2> $LOCAL_WORKDIR/traf_temp_output
if [ $? != 0 ]; then
   # ok if directory already exists
   dir_exists=$(grep "File exists" $LOCAL_WORKDIR/traf_temp_output | wc -l)
   if [ $dir_exists -eq 0 ]; then
      echo "***ERROR: 'hadoop fs -mkdir /lobs' command failed"
      echo "***ERROR: $(cat $LOCAL_WORKDIR/traf_temp_output)"
      exit -1
   fi
fi
sudo su hdfs --command "hadoop fs -chown -R $TRAF_USER:trafodion /lobs"

#=====================================
# Modify hadoop settings as needed by Trafodion

rm $LOCAL_WORKDIR/traf_hdfs1_config_temp 2> /dev/null
rm $LOCAL_WORKDIR/traf_hbase_config_temp 2> /dev/null

# change the hdfs configuration using Cloudera's REST API
curl -X PUT -H 'Content-Type:application/json' -u $ADMIN:$PASSWORD  --data \
'{ "roleTypeConfigs" :  [ {
        "roleType" : "NAMENODE",
        "items": [ {
                "name" : "namenode_java_heapsize",
        "value" : "1073741824"
                } ]
   }, {
        "roleType" : "SECONDARYNAMENODE",
        "items":[ {
                "name" : "secondary_namenode_java_heapsize",
        "value" : "1073741824"
                } ]
     } ],
    "items": [ {
             "name":"dfs_namenode_acls_enabled",
             "value":"true"
             } ]
}' \
http://$URL/api/v1/clusters/$CLUSTER_NAME/services/hdfs/config > $LOCAL_WORKDIR/traf_hdfs_config_temp

if [ $? != 0 ]; then
    echo "***ERROR: Unable to modify HDFS configuration through Cloudera's REST API."
    echo "***ERROR: Check if Cloudera URL is correct, may need to enter external IP address."
    echo "***ERROR: Check that HDFS is running without error."
    exit -1
fi

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE $LOCAL_WORKDIR/traf_hdfs_config_temp | grep Error | wc -l)
if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to modify hdfs configuration through Cloudera's REST API."
    echo "***ERROR: Check if Cloudera URL is correct, may need to enter external IP address
." 
    echo "***ERROR: Check that HDFS is running without error."
    exit -1
fi

rm $LOCAL_WORKDIR/traf_hdfs_config_temp 2> /dev/null

# change the hbase configuration using Cloudera Manager's REST api
# NOTE: hbase.regionserver.lease.period is used as it is equivalent to
#       hbase.client.scanner.timeout.period and Cloudera only allows
#       hbase.regionserver.lease.period to be set through the REST API.
curl -X PUT -H 'Content-Type:application/json' -u $ADMIN:$PASSWORD  --data \
'{ "roleTypeConfigs" :	[ {
	"roleType" : "MASTER",
	"items" : [ { 
		"name" : "hbase_master_config_safety_valve", 
        "value" : "<property>\r\n   <name>hbase.master.distributed.log.splitting</name>\r\n   <value>false</value>\r\n</property>\r\n <property>\r\n   <name>hbase.snapshot.master.timeoutMillis</name>\r\n   <value>600000</value>\r\n</property>\r\n"
		} ]
    }, {
	"roleType" : "REGIONSERVER", 
	"items" : [ { 
		"name" : "hbase_coprocessor_region_classes", 
                "value" : "org.apache.hadoop.hbase.coprocessor.transactional.TrxRegionObserver,org.apache.hadoop.hbase.coprocessor.transactional.TrxRegionEndpoint,org.apache.hadoop.hbase.coprocessor.AggregateImplementation"
		}, {
		"name" : "hbase_regionserver_lease_period", 
		"value" : "600000"
		}, {
		"name" : "hbase_regionserver_config_safety_valve", 
		"value" : "<property>\r\n   <name>hbase.hregion.impl</name>\r\n   <value>org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion</value>\r\n</property>\r\n <property>\r\n   <name>hbase.regionserver.region.split.policy</name>\r\n   <value>org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy</value>\r\n</property>\r\n  <property>\r\n   <name>hbase.snapshot.enabled</name>\r\n   <value>true</value>\r\n</property>\r\n <property>\r\n   <name>hbase.bulkload.staging.dir</name>\r\n   <value>/hbase-staging</value>\r\n</property>\r\n <property>\r\n   <name>hbase.regionserver.region.transactional.tlog</name>\r\n   <value>true</value>\r\n</property>\r\n <property>\r\n   <name>hbase.snapshot.region.timeout</name>\r\n   <value>600000</value>\r\n</property>\r\n "
		} ] 
	} ] 
}' \
http://$URL/api/v1/clusters/$CLUSTER_NAME/services/$HBASE/config > $LOCAL_WORKDIR/traf_hbase_config_temp

if [ $? != 0 ]; then
    echo "***ERROR: Unable to modify HBase configuration through Cloudera's REST API."
    echo "***ERROR: Check that HBase is running without error."
    exit -1
fi

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE $LOCAL_WORKDIR/traf_hbase_config_temp | grep Error | wc -l)
if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to modify HBase configuration through Cloudera's REST API."
    echo "***ERROR: Check that HBase is running without error."
    cat $LOCAL_WORKDIR/traf_hbase_config_temp
    exit -1
fi
curl_error=$(grep message $LOCAL_WORKDIR/traf_hbase_config_temp | wc -l)
if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to modify HBase configuration through Cloudera's REST API."
    echo "***ERROR: Check that HBase is running without error."
    cat $LOCAL_WORKDIR/traf_hbase_config_temp
    exit -1
fi
rm $LOCAL_WORKDIR/traf_hbase_config_temp 2> /dev/null

#=====================================
# restart Cloudera to pick up all the changes just made
poll_time=30
echo "***INFO: restarting Hadoop to pickup Trafodion transaction jar"
echo "***INFO: ...polling every $poll_time seconds until restart is completed."
restart_info=$(curl -X POST -u $ADMIN:$PASSWORD \
    http://$URL/api/v1/clusters/$CLUSTER_NAME/commands/restart)
echo $restart_info
command_id=$(echo $restart_info | grep id | awk '{print $4}' | sed -e 's@,@@' )
echo "***DEBUG: Cloudera command_id=$command_id"

# poll until restart is completed as a restart can take a while
active=1
while [ $active -ne 0 ]; do
    sleep $poll_time
    curl -u $ADMIN:$PASSWORD \
        http://$URL/api/v1/commands/$command_id \
        > $LOCAL_WORKDIR/hbase_restart_status_temp
    cat $LOCAL_WORKDIR/hbase_restart_status_temp
    echo "***INFO: ...polling every $poll_time seconds until restart is completed."
    # if restart command is still active, then active will not equal 0
    active=$(cat $LOCAL_WORKDIR/hbase_restart_status_temp | grep '"active" : true' | wc -l)
done

# make sure restart completed successfully
failures=$(cat $LOCAL_WORKDIR/hbase_restart_status_temp | grep '"success" : false' | wc -l)
if [ $failures -ne 0 ]; then
    echo "***ERROR: Unable to restart Hadoop."
    exit -1
fi

echo "***INFO: Hadoop restart completed successfully"

# wait to make sure HDFS is fully restarted and out of safemode
echo "***INFO: waiting for HDFS to exit safemode"
sudo su hdfs --command "hdfs dfsadmin -safemode wait"

#====================================================
# NOTE: These command must be done AFTER acls are 
#       enabled and HDFS has been restarted
echo "***INFO: Setting HDFS ACLs for snapshot scan support"
sudo su hdfs --command "hdfs dfs -mkdir -p /hbase/archive"
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -mkdir -p /hbase/archive) command failed"
   exit -1
fi
sudo su hdfs --command "hdfs dfs -chown hbase:hbase /hbase/archive"
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -chown hbase:hbase /hbase/archive) command failed"
   exit -1
fi
sudo su hdfs --command "hdfs dfs -setfacl -R -m user:$TRAF_USER:rwx /hbase/archive"
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m user:$TRAF_USER:rwx /hbase/archive) command failed"
   exit -1
fi
sudo su hdfs --command "hdfs dfs -setfacl -R -m default:user:$TRAF_USER:rwx /hbase/archive"
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m default:user:$TRAF_USER:rwx /hbase/archive) command failed"
   exit -1
fi
sudo su hdfs --command "hdfs dfs -setfacl -R -m mask::rwx /hbase/archive"
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m mask::rwx /hbase/archive) command failed"
   exit -1
fi


