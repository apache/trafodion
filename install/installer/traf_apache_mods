#!/bin/bash
# @@@ START COPYRIGHT @@@
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# @@@ END COPYRIGHT @@@
#
# This script will configure HBase with HBase-trx
# and co-processors needed for Trafodion.  It uses
# Ambari's configs.sh script to do this.
#
# NOTE: Only for Ambari installations

TRAF_CONFIG=/etc/trafodion/trafodion_config
source $TRAF_CONFIG

export PDSH="pdsh -R exec"
export PDSH_SSH_CMD="ssh -q -n %h"
export PDCP="pdcp -R ssh"

export PDSH_HADOOP_NODES="$PDSH $MY_HBASE_NODES $PDSH_SSH_CMD"
export PDCP_HADOOP_NODES="$PDCP $MY_HBASE_NODES"
#=====================================
# copy Trafodion hbase trx jar to /usr/lib/hbase/lib

cd $UNTAR_DIR

HDFS_NODE=$(echo $HDFS_NODES | head -n1 | awk '{print $1;}')
HBASE_NODE=$(echo $HBASE_NODES | head -n1 | awk '{print $1;}')
echo "export HDFS_NODE=\"$HDFS_NODE\"" >> $TRAF_CONFIG
echo "export HBASE_NODE=\"$HBASE_NODE\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
source $TRAF_CONFIG


hbase_trx_jar="hbase-trx-apache1_2*.jar"
if [[ $hbaseVersion =~ "1.1" ]]; then
   hbase_trx_jar="hbase-trx-apache1_1*.jar"
elif [[ $hbaseVersion =~ "1.0" ]]; then
   hbase_trx_jar="hbase-trx-apache1_0*.jar"
fi

traf_util_jar="trafodion-utility-*.jar"


# The permissions the Trafodion build process creates on the hbase-trx jar
# files does not work well with the installation process so we change them
sudo chmod -R 777 $UNTAR_DIR/export/lib

if [ ! -f $UNTAR_DIR/export/lib/$hbase_trx_jar ]; then
    echo "***ERROR: unable to find $UNTAR_DIR/export/lib/$hbase_trx_jar"
    exit -1
fi

# if more than one node then copy to all nodes
echo "***INFO: copying $hbase_trx_jar to all nodes"
if [ $node_count -ne 1 ]; then
    $PDSH_HADOOP_NODES sudo rm -rf  $HBASE_HOME/lib/hbase-trx* 2>/dev/null
    $TRAF_PDSH mkdir -p $LOCAL_WORKDIR 2>/dev/null
    $PDSH_HADOOP_NODES mkdir -p $LOCAL_WORKDIR 2>/dev/null
    cp $UNTAR_DIR/export/lib/$hbase_trx_jar $LOCAL_WORKDIR
    cp $UNTAR_DIR/export/lib/$traf_util_jar $LOCAL_WORKDIR
    $PDCP_HADOOP_NODES $LOCAL_WORKDIR/$hbase_trx_jar $LOCAL_WORKDIR
    $PDCP_HADOOP_NODES $LOCAL_WORKDIR/$traf_util_jar $LOCAL_WORKDIR
    $PDSH_HADOOP_NODES sudo cp $LOCAL_WORKDIR/$traf_util_jar $HBASE_HOME/lib
    $PDSH_HADOOP_NODES sudo cp $LOCAL_WORKDIR/$hbase_trx_jar $HBASE_HOME/lib
    $PDSH_HADOOP_NODES sudo chmod 644 $HBASE_HOME/lib/$hbase_trx_jar
    $PDSH_HADOOP_NODES sudo chmod 644 $HBASE_HOME/lib/$traf_util_jar

    $PDSH_HADOOP_NODES rm $LOCAL_WORKDIR/$hbase_trx_jar 2>/dev/null
    $PDSH_HADOOP_NODES rm $LOCAL_WORKDIR/$traf_util_jar 2>/dev/null
else
    for node in $HBASE_NODES
    do 
    ssh -q -n $node sudo rm -rf $HBASE_HOME/lib/hbase-trx* 2>/dev/null
    ssh -q -n $node sudo mkdir -p $TRAF_WORKDIR 2>/dev/null
    ssh -q -n $node sudo chmod 777 $TRAF_WORKDIR
    scp -q $UNTAR_DIR/export/lib/$hbase_trx_jar $(whoami)@$node:$TRAF_WORKDIR
    scp -q $UNTAR_DIR/export/lib/$traf_util_jar $(whoami)@$node:$TRAF_WORKDIR
    ssh -q -n $node sudo cp $TRAF_WORKDIR/$hbase_trx_jar $HBASE_HOME/lib
    ssh -q -n $node sudo cp $TRAF_WORKDIR/$traf_util_jar $HBASE_HOME/lib
    ssh -q -n $node sudo chmod 644 $HADOOP_PATH/$hbase_trx_jar
    ssh -q -n $node sudo chmod 644 $HADOOP_PATH/$traf_util_jar
    done
fi

#=======================================
#Check that HBase-trx copied to all nodes

for node in $HBASE_NODES
do
   copiedOver=$(ssh -q -n $node sudo ls $HBASE_HOME/lib/hbase-trx* | wc -l)
   if [[ $copiedOver -ne "1" ]]; then
      echo "***ERROR: $hbase_trx_jar was not copied on $node"
      echo "***ERROR: Please investigate why this happened"
      echo "***ERROR: Trafodion can not start without this. EXITING..."
      exit -1
   fi
done

echo "***INFO: $hbase_trx_jar copied correctly! Huzzah."



#Copy hbase-site.xml file
ssh -q -n $HBASE_NODE sudo cp $HBASE_HOME/conf/hbase-site.xml $HOME
ssh -q -n $HBASE_NODE sudo chown $(whoami).$(whoami) $HOME/hbase-site.xml
ssh -q -n $HBASE_NODE sudo chmod 777 $HOME/hbase-site.xml

scp -q $(whoami)@$HBASE_NODE:$HOME/hbase-site.xml $HOME
if [[ $? -gt 1 ]]; then
   echo "***ERROR: Unable to find $HBASE_HOME/conf/hbase-site.xml file on $HBASE_NODE or unable to copy."
   exit -1
fi
sudo cp $HOME/hbase-site.xml $TRAF_WORKDIR
sudo chown trafodion.trafodion $TRAF_WORKDIR/hbase-site.xml

#=====================================
# create new directories for bulkload and lobs if not already there
rm $LOCAL_WORKDIR/traf_temp_output 2>/dev/null

ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -mkdir -p /user/'"$TRAF_USER"'"'
if [ $? != 0 ]; then
   echo "***ERROR: hds dfs -mkdir -p /hbase-staging' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -chown -R '"$TRAF_USER"' /user/'"$TRAF_USER"'"'

ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -mkdir -p /hbase-staging"'
if [ $? != 0 ]; then
   echo "***ERROR: hds dfs -mkdir -p /hbase-staging' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -chown -R' "$HBASE_USER"':'"$HBASE_GROUP" '/hbase-staging"'
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -mkdir -p /bulkload"'
if [ $? != 0 ]; then
   echo "***ERROR: 'hdfs dfs -mkdir -p /bulkload' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command " ' "$HADOOP_PREFIX"'/bin/hdfs dfs -chown -R' "$TRAF_USER"':trafodion /bulkload"'

# Create lobs directory
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -mkdir -p /lobs"'
if [ $? != 0 ]; then
   echo "***ERROR: 'hdfs dfs -mkdir -p /lobs' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -chown -R' "$TRAF_USER"':trafodion /lobs"'

ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -mkdir -p /trafodion_backups" 2> $HOME/traf_temp_output'
if [ $? != 0 ]; then
   echo "***ERROR: 'hdfs dfs -mkdir -p /trafodion_backups' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -chown -R' "$TRAF_USER"':trafodion /trafodion_backups"'


ssh -q -n $HDFS_NODE 'rm -rf $HOME/traf_temp_output'
#=====================================
#Restart HAdoop

echo "****INFO: Stopping HDFS, HBASE, and Zookeeper" 



sudo su $HBASE_USER $HBASE_HOME/bin/stop-hbase.sh

sudo su $ZOO_USER $ZOO_HOME/bin/zkServer.sh stop

sudo su $HDFS_USER $HADOOP_PREFIX/sbin/stop-dfs.sh


echo "***IMPORTANT***********************************************************************"
echo 
echo "***INFO: Settings below need to be set for Trafodion to work on Apache Hadoop/HBase"
echo "***INFO: In hdfs-site.xml set dfs.namenode.acls.enabled to true and dfs.umaskmode to 0002"
echo "***INFO: In zoo.cfg set maxClientCnxns to 0"
echo "***INFO: Create $ZOO_HOME/conf/zookeeeper-env.sh and set JAVA_HOME"
echo "***INFO: In hbase-site.xml set hbase.coprocessor.region.classes to org.apache.hadoop.hbase.coprocessor.transactional.TrxRegionObserver, org.apache.hadoop.hbase.coprocessor.transactional.TrxRegionEndpoint, org.apache.hadoop.hbase.coprocessor.AggregateImplementation"
echo "***INFO: In hbase-site.xml set hbase.hregion.impl to org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion"
echo 
echo "***IMPORTANT***********************************************************************"

sleep 15



sudo su $ZOO_USER $ZOO_HOME/bin/zkServer.sh start

sudo su $HDFS_USER $HADOOP_PREFIX/sbin/start-dfs.sh

ssh -q -n $HDFS_NODE 'sudo su ' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfsadmin -safemode wait"'

sudo su $HBASE_USER $HBASE_HOME/bin/start-hbase.sh

#=====================================
# NOTE: These command must be done AFTER acls are 
#       enabled and HDFS has been restarted
echo "***INFO: Setting HDFS ACLs for snapshot scan support"
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -mkdir -p /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -mkdir -p /apps/hbase/data/archive) command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -chown ' "$HBASE_USER"':'"$HDFS_USER" '/apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -chown hbase:hdfs /apps/hbase/data/archive) command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -setfacl -R -m user:'"$TRAF_USER"':rwx /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m mask::rwx /apps/hbase/data/archive) command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -setfacl -R -m default:user:'"$TRAF_USER"':rwx /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m mask::rwx /apps/hbase/data/archive) command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "' "$HADOOP_PREFIX"'/bin/hdfs dfs -setfacl -R -m mask::rwx /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m mask::rwx /apps/hbase/data/archive) command failed"
   exit -1
fi


MODS_COMPLETE="Y"
sudo chmod 777 $TRAF_CONFIG
sed -i '/MODS_COMPLETE\=/d' $TRAF_CONFIG
echo "export MODS_COMPLETE=\"$MODS_COMPLETE\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
source $TRAF_CONFIG

TRAF_CONFIG_FILE="trafodion_config"
TRAF_CONFIG_DIR="/etc/trafodion"

if [ $node_count -ne 1 ]; then
   cp $TRAF_CONFIG $LOCAL_WORKDIR
   $TRAF_PDCP $LOCAL_WORKDIR/$TRAF_CONFIG_FILE $HOME
   $TRAF_PDSH sudo mkdir -p $TRAF_CONFIG_DIR
   $TRAF_PDSH sudo cp $HOME/$TRAF_CONFIG_FILE $TRAF_CONFIG_DIR
   $TRAF_PDSH sudo chmod 777 $TRAF_CONFIG
fi

