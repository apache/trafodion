#!/bin/bash
# @@@ START COPYRIGHT @@@
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# @@@ END COPYRIGHT @@@
#

source /etc/trafodion/trafodion_config
TRAF_CONFIG=/etc/trafodion/trafodion_config


curlRC=0
if [ $HADOOP_TYPE == "cloudera" ]; then
   curl -k -su $ADMIN:$PASSWORD $URL/api/v10/hosts > tempFile
   curlRC=$?
   numberHadoopNodes=$(grep -r "hostname" tempFile | wc -l)
   grep -r "hostname" tempFile > tempFile2
   grep -r "hostId" tempFile > tempHadoop

   if [ -d /opt/cloudera/parcels/CDH ]; then
      export HADOOP_PATH="/opt/cloudera/parcels/CDH/lib/hbase/lib"
      export HADOOP_BIN_PATH="/opt/cloudera/parcels/CDH/bin"
   else
      export HADOOP_PATH="/usr/lib/hbase/lib"
      export HADOOP_BIN_PATH="/usr/bin"
   fi
  echo "***INFO: HADOOP_PATH=$HADOOP_PATH"
  echo "***INFO: HADOOP_BIN_PATH=$HADOOP_BIN_PATH"

  HADOOP_IDS=""
  HBASE_NODES=""
  MY_HBASE_NODES=""
  HDFS_NODES=""
  MY_HDFS_NODES=""
  while read line
  do
     hostName=$(echo $line | awk '{print $3}' | sed 's/\"//g' | sed 's/\..*//' | sed 's/\,//g')
     HADOOP_IDS="$HADOOP_IDS $hostName"
   done < tempHadoop

   for id in $HADOOP_IDS
   do
      hasHBase=$(curl -k -su $ADMIN:$PASSWORD $URL/api/v10/hosts/$id | grep "serviceName" | grep -i hbase | wc -l)
      if [[ "$hasHBase" -ge "1" ]]; then
         name=$(curl -k -su $ADMIN:$PASSWORD $URL/api/v10/hosts/$id | grep "hostname")
         hostName=$(echo $name | awk '{print $3}' | sed 's/\"//g' | sed 's/\..*//' | sed 's/\,//g')
         HBASE_NODES="$HBASE_NODES $hostName"
         MY_HBASE_NODES="$MY_HBASE_NODES -w $hostName"
      fi
      
      hasHDFS=$(curl -k -su $ADMIN:$PASSWORD $URL/api/v10/hosts/$id | grep "serviceName" | grep -i hdfs | wc -l)
      if [[ "$hasHDFS" -ge "1" ]]; then
         name=$(curl -k -su $ADMIN:$PASSWORD $URL/api/v10/hosts/$id | grep "hostname")
         hostName=$(echo $name | awk '{print $3}' | sed 's/\"//g' | sed 's/\..*//' | sed 's/\,//g')
         HDFS_NODES="$HDFS_NODES $hostName"
         MY_HDFS_NODES="$MY_HDFS_NODES -w $hostName"
      fi 
   done

   HADOOP_NODES=""
   MY_HADOOP_NODES=""
   while read line
   do
      hostName=$(echo $line | awk '{print $3}' | sed 's/\"//g' | sed 's/\..*//' | sed 's/\,//g')
      HADOOP_NODES="$HADOOP_NODES $hostName"
      MY_HADOOP_NODES="$MY_HADOOP_NODES -w $hostName"
   done < tempFile2
   rm tempFile2
fi

if [[ $HADOOP_TYPE == "hortonworks" ]]; then
   curl -k -su $ADMIN:$PASSWORD $URL/api/v1/clusters/$CLUSTER_NAME/hosts > tempFile
   curlRC=$?
   numberHadoopNodes=$(grep -r "host_name" tempFile | wc -l)
   grep -r "host_name" tempFile > tempFile2
      
   if [ -d /usr/lib/hbase/lib ]; then
      HADOOP_PATH="/usr/lib/hbase/lib"
   else
      HADOOP_PATH="/usr/hdp/current/hbase-regionserver/lib"
   fi
   echo "***INFO: HADOOP_PATH=$HADOOP_PATH"

   HDFS_NODES=""
   MY_HDFS_NODES=""
   HBASE_NODES=""
   MY_HBASE_NODES=""
   
   HADOOP_NODES=""
   MY_HADOOP_NODES=""
   while read line
   do
      hostName=$(echo $line | awk '{print $3}' | sed 's/\"//g' | sed 's/\,//g')
      HADOOP_NODES="$HADOOP_NODES $hostName"
      MY_HADOOP_NODES="$MY_HADOOP_NODES -w $hostName"
   done < tempFile2

   for node in $HADOOP_NODES
   do
       hasHBase=$(curl -k -su $ADMIN:$PASSWORD $URL/api/v1/clusters/$CLUSTER_NAME/hosts/$node | grep component_name| grep -i hbase | wc -l)
       if [[ "$hasHBase" -ge "1" ]]; then
          HBASE_NODES="$HBASE_NODES $node"
          MY_HBASE_NODES="$MY_HBASE_NODES -w $node"
       fi

       hasHDFS=$(curl -k -su $ADMIN:$PASSWORD $URL/api/v1/clusters/$CLUSTER_NAME/hosts/$node | grep component_name | grep -i datanode | wc -l)
       hasHDFS1=$(curl -k -su $ADMIN:$PASSWORD $URL/api/v1/clusters/$CLUSTER_NAME/hosts/$node | grep component_name | grep -i namenode | wc -l)
       
       if [[ "$hasHDFS" -ge "1" ]] || [[ "$hasHDFS1" -ge "1" ]]; then
          HDFS_NODES="$HDFS_NODES $node"
          MY_HDFS_NODES="$MY_HDFS_NODES -w $node"
       fi
        
   done
fi

if [ $curlRC != 0 ]; then
   echo "***ERROR: Unable to get list of hosts from $HADOOP_TYPE (RC=$curlRC)"
   echo "***ERROR: curl command failed."
   exit -1
fi

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE tempFile | grep Error | wc -l)

if [ $curl_error -ne 0 ]; then
   echo "***ERROR: Unable to get list of hosts from $HADOOP_TYPE"
   cat tempFile
   exit -1
fi

if [[ -z $HADOOP_NODES ]]; then
   echo "***ERROR: List of $HADOOP_TYPE nodes not found."
   echo "***ERROR: Check that $HADOOP_TYPE is up and running."
   echo "***INFO: $HADOOP_TYPE list of nodes: $HADOOP_NODES"
   exit -1
fi

echo "***INFO: $HADOOP_TYPE list of nodes: $HADOOP_NODES"
echo "***INFO: $HADOOP_TYPE list of HDFS nodes: $HDFS_NODES"
echo "***INFO: $HADOOP_TYPE list of HBASE nodes: $HBASE_NODES"



hadoop_node_count=$(echo $HADOOP_NODES | wc -w)

sudo chmod 777 $TRAF_CONFIG
sed -i '/HADOOP_NODES\=/d' $TRAF_CONFIG
echo "export HADOOP_NODES=\"$HADOOP_NODES\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
sed -i '/MY_HADOOP_NODES\=/d' $TRAF_CONFIG
echo "export MY_HADOOP_NODES=\"$MY_HADOOP_NODES\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
sed -i '/HDFS_NODES\=/d' $TRAF_CONFIG
echo "export HDFS_NODES=\"$HDFS_NODES\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
sed -i '/MY_HDFS_NODES\=/d' $TRAF_CONFIG
echo "export MY_HDFS_NODES=\"$MY_HDFS_NODES\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
sed -i '/HBASE_NODES\=/d' $TRAF_CONFIG
echo "export HBASE_NODES=\"$HBASE_NODES\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
sed -i '/MY_HBASE_NODES\=/d' $TRAF_CONFIG
echo "export MY_HBASE_NODES=\"$MY_HBASE_NODES\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
sed -i '/HADOOP_PATH\=/d' $TRAF_CONFIG
echo "export HADOOP_PATH=\"$HADOOP_PATH\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
sed -i '/HADOOP_BIN_PATH\=/d' $TRAF_CONFIG
echo "export HADOOP_BIN_PATH=\"$HADOOP_BIN_PATH\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
sed -i '/hadoop_node_count\=/d' $TRAF_CONFIG
echo "export hadoop_node_count=\"$hadoop_node_count\"" >> $TRAF_CONFIG

for node in $HADOOP_NODES
do
   ssh -q -n $node echo "***INFO: Testing ssh on $node"
   if [[ $? -ne "0" ]]; then
      errorFound=1
      ERROR_NODES="$ERROR_NODES $node"
   fi
done

if [[ $errorFound == "1" ]]; then
   echo "***ERROR: Could not ssh to $ERROR_NODES."
   echo "***ERROR: Check permissions and known hosts files."
   exit -1
fi

for node in $HADOOP_NODES
do
   ssh -q -n $node sudo echo "***INFO: Testing sudo access on $node"
   if [ $? -ne "0" ]; then
      error=1
      ERROR_NODES="$ERROR_NODES $newNode"
   fi
done

if [[ $error == "1" ]]; then
   echo "***ERROR: $ERROR_NODES does not have sudo access."
   echo "***ERROR: Must have sudo access on all nodes."
   exit -1
fi


rm -rf tempFile*
