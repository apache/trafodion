#!/bin/bash
# @@@ START COPYRIGHT @@@
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# @@@ END COPYRIGHT @@@
#
# This script will configure HBase with HBase-trx
# and co-processors needed for Trafodion.  It uses
# Ambari's configs.sh script to do this.
#
# NOTE: Only for Ambari installations

TRAF_CONFIG=/etc/trafodion/trafodion_config
source $TRAF_CONFIG

export PDSH="pdsh -R exec"
export PDSH_SSH_CMD="ssh -q -n %h"
export PDCP="pdcp -R ssh"

export PDSH_HADOOP_NODES="$PDSH $MY_HBASE_NODES $PDSH_SSH_CMD"
export PDCP_HADOOP_NODES="$PDCP $MY_HBASE_NODES"

#=====================================
# create principals for hdfs user if secure hadoop is enabled
if [[ $SECURE_HADOOP == "Y" ]]; then
   echo "***INFO: obtain and cache the Kerberos ticket-granting ticket for hdfs user"
   HDFS_PRINCIPAL=$(sudo klist -kte $HDFS_KEYTAB | awk '{print $4}' | uniq)
   echo "  using keytab $HDFS_KEYTAB and principal $HDFS_PRINCIPAL"
   sudo -u $HDFS_USER kinit -kt $HDFS_KEYTAB $HDFS_PRINCIPAL
   if [ $? != 0 ]; then
      echo "***ERROR: failed to obtain and cache the Kerberos ticket-granting ticket for hdfs user"
      exit -1
   fi
fi

#=====================================
# copy Trafodion hbase trx jar to /usr/lib/hbase/lib

cd $UNTAR_DIR

PORT=`echo $URL | sed 's/.*://'`
AMBARI_HOST=$(echo $URL | sed 's@.*://@@' | sed 's@:.*@@')
HDFS_NODE=$(echo $HDFS_NODES | head -n1 | awk '{print $1;}')
HBASE_NODE=$(echo $HBASE_NODES | head -n1 | awk '{print $1;}')
echo "export AMBARI_HOST=\"$AMBARI_HOST\"" >> $TRAF_CONFIG
echo "export HDFS_NODE=\"$HDFS_NODE\"" >> $TRAF_CONFIG
echo "export HBASE_NODE=\"$HBASE_NODE\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
source $TRAF_CONFIG

#determine java version and choose corresponding jar files
if [[ $CDH_5_3_HDP_2_2_SUPPORT == "N" ]]; then
   if [[ $HDP_2_3_SUPPORT == "Y" ]]; then
      hbase_trx_jar="hbase-trx-hdp2_3-*.jar"
   else
      hbase_trx_jar="hbase-trx-hdp2_1-*.jar"
   fi
else
   hbase_trx_jar="hbase-trx-hdp2_2-*.jar"
fi

traf_util_jar="trafodion-utility-*.jar"


# The permissions the Trafodion build process creates on the hbase-trx jar
# files does not work well with the installation process so we change them
sudo chmod -R 777 $UNTAR_DIR/export/lib

#if [ ! -f $UNTAR_DIR/export/lib/$hbase_trx_jar ]; then
#    echo "***ERROR: unable to find $UNTAR_DIR/export/lib/$hbase_trx_jar"
#    exit -1
#fi

# if more than one node then copy to all nodes
echo "***INFO: copying $hbase_trx_jar to all nodes"
if [ $node_count -ne 1 ]; then
    $PDSH_HADOOP_NODES sudo rm -rf  $HADOOP_PATH/hbase-trx* 2>/dev/null
    $PDSH_HADOOP_NODES sudo rm -rf /usr/lib/hbase/lib/hbase-trx* 2>/dev/null
    $PDSH_HADOOP_NODES sudo rm -rf /usr/hdp/current/hbase-regionserver/lib/hbase-trx* 2>/dev/null
    $PDSH_HADOOP_NODES sudo rm -rf /usr/share/cmf/lib/plugins/hbase-trx* 2>/dev/null
    $PDSH_HADOOP_NODES sudo rm -rf /usr/lib/hbase/lib/trafodion* 2>/dev/null
    $PDSH_HADOOP_NODES sudo rm -rf /usr/hdp/current/hbase-regionserver/lib/trafodion* 2>/dev/null
    $PDSH_HADOOP_NODES sudo rm -rf /usr/share/cmf/lib/plugins/trafodion* 2>/dev/null
    $TRAF_PDSH mkdir -p $LOCAL_WORKDIR 2>/dev/null
    $PDSH_HADOOP_NODES mkdir -p $LOCAL_WORKDIR 2>/dev/null
    cp $UNTAR_DIR/export/lib/$hbase_trx_jar $LOCAL_WORKDIR
    cp $UNTAR_DIR/export/lib/$traf_util_jar $LOCAL_WORKDIR
    $PDCP_HADOOP_NODES $LOCAL_WORKDIR/$hbase_trx_jar $LOCAL_WORKDIR
    $PDCP_HADOOP_NODES $LOCAL_WORKDIR/$traf_util_jar $LOCAL_WORKDIR
    $PDSH_HADOOP_NODES sudo cp $LOCAL_WORKDIR/$traf_util_jar $HADOOP_PATH
    $PDSH_HADOOP_NODES sudo cp $LOCAL_WORKDIR/$hbase_trx_jar $HADOOP_PATH
    $PDSH_HADOOP_NODES sudo chmod 644 $HADOOP_PATH/$hbase_trx_jar
    $PDSH_HADOOP_NODES sudo chmod 644 $HADOOP_PATH/$traf_util_jar

    $PDSH_HADOOP_NODES rm $LOCAL_WORKDIR/$hbase_trx_jar 2>/dev/null
    $PDSH_HADOOP_NODES rm $LOCAL_WORKDIR/$traf_util_jar 2>/dev/null
else
    for node in $HBASE_NODES
    do 
    ssh -q -n $node sudo rm -rf $HADOOP_PATH/hbase-trx* 2>/dev/null
    ssh -q -n $node sudo rm -rf /usr/lib/hbase/lib/hbase-trx* 2>/dev/null
    ssh -q -n $node sudo rm -rf /usr/share/cmf/lib/plugins/hbase-trx* 2>/dev/null
    ssh -q -n $node sudo rm -rf /usr/lib/hbase/lib/trafodion* 2>/dev/null
    ssh -q -n $node sudo rm -rf /usr/share/cmf/lib/plugins/trafodion* 2>/dev/null
    ssh -q -n $node sudo mkdir -p $TRAF_WORKDIR 2>/dev/null
    ssh -q -n $node sudo chmod 777 $TRAF_WORKDIR
    scp -q $UNTAR_DIR/export/lib/$hbase_trx_jar $(whoami)@$node:$TRAF_WORKDIR
    scp -q $UNTAR_DIR/export/lib/$traf_util_jar $(whoami)@$node:$TRAF_WORKDIR
    ssh -q -n $node sudo cp $TRAF_WORKDIR/$hbase_trx_jar $HADOOP_PATH
    ssh -q -n $node sudo cp $TRAF_WORKDIR/$traf_util_jar $HADOOP_PATH
    ssh -q -n $node sudo chmod 644 $HADOOP_PATH/$hbase_trx_jar
    ssh -q -n $node sudo chmod 644 $HADOOP_PATH/$traf_util_jar
    done
fi

#=======================================
#Check that HBase-trx copied to all nodes

for node in $HBASE_NODES
do
   copiedOver=$(ssh -q -n $node sudo ls $HADOOP_PATH/hbase-trx* | wc -l)
   if [[ $copiedOver -ne "1" ]]; then
      echo "***ERROR: $hbase_trx_jar was not copied on $node"
      echo "***ERROR: Please investigate why this happened"
      echo "***ERROR: Trafodion can not start without this. EXITING..."
      exit -1
   fi
done

echo "***INFO: $hbase_trx_jar copied correctly! Huzzah."



#Copy hbase-site.xml file
ssh -q -n $HBASE_NODE sudo cp /etc/hbase/conf/hbase-site.xml $HOME
ssh -q -n $HBASE_NODE sudo chown $(whoami).$(whoami) $HOME/hbase-site.xml
ssh -q -n $HBASE_NODE sudo chmod 777 $HOME/hbase-site.xml

scp -q $(whoami)@$HBASE_NODE:$HOME/hbase-site.xml $HOME
if [[ $? -gt 1 ]]; then
   echo "***ERROR: Unable to find /etc/hbase/conf/hbase-site.xml file on $HBASE_NODE or unable to copy."
   exit -1
fi
sudo cp $HOME/hbase-site.xml $TRAF_WORKDIR
sudo chown trafodion.trafodion $TRAF_WORKDIR/hbase-site.xml

#=====================================
# create new directories for bulkload and lobs if not already there
rm $LOCAL_WORKDIR/traf_temp_output 2>/dev/null

ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "hadoop fs -mkdir -p /user/'"$TRAF_USER"'"'
if [ $? != 0 ]; then
   echo "***ERROR: 'hadoop fs -mkdir -p /user/$TRAF_USER' command failed"
   exit -1
fi

ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "hadoop fs -chown -R '"$TRAF_USER"' /user/'"$TRAF_USER"'"'

ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "hadoop fs -mkdir -p /hbase-staging"'
if [ $? != 0 ]; then
   echo "***ERROR: 'hadoop fs -mkdir -p /hbase-staging' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "hadoop fs -chown -R' "$HBASE_USER"':'"$HBASE_GROUP" '/hbase-staging"'
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "hadoop fs -mkdir -p /bulkload"'
if [ $? != 0 ]; then
   echo "***ERROR: 'hadoop fs -mkdir -p /bulkload' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "hadoop fs -chown -R' "$TRAF_USER"':trafodion /bulkload"'

# Create lobs directory
ssh -q -n $HDFS_NODE 'sudo su hdfs --command "hadoop fs -mkdir -p /lobs"'
if [ $? != 0 ]; then
   echo "***ERROR: 'hadoop fs -mkdir -p /lobs' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su hdfs --command "hadoop fs -chown -R' "$TRAF_USER"':trafodion /lobs"'

ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "hadoop fs -mkdir -p /trafodion_backups" 2> $HOME/traf_temp_output'
if [ $? != 0 ]; then
   echo "***ERROR: 'hadoop fs -mkdir -p /trafodion_backups' command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su' "$HDFS_USER" '--command "hadoop fs -chown -R' "$TRAF_USER"':trafodion /trafodion_backups"'


ssh -q -n $HDFS_NODE 'rm -rf $HOME/traf_temp_output'
#=====================================
# change the hbase configuration using Ambari's script

AMBARI_DIR=/var/lib/ambari-server/resources/scripts
cd $LOCAL_WORKDIR

# Bug: install removes hbase co-processors installed as part of kerberozing Hadoop.  Add them
# as part of adding trafodion co-processor (add JIRA #)
if [[ "$SECURE_HADOOP" == "Y" ]]; then
   ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hbase-site hbase.coprocessor.region.classes "org.apache.hadoop.hbase.coprocessor.transactional.TrxRegionObserver,org.apache.hadoop.hbase.coprocessor.transactional.TrxRegionEndpoint,org.apache.hadoop.hbase.coprocessor.AggregateImplementation,org.apache.hadoop.hbase.security.access.AccessController,org.apache.hadoop.hbase.security.token.TokenProvider"'
else
   ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hbase-site hbase.coprocessor.region.classes "org.apache.hadoop.hbase.coprocessor.transactional.TrxRegionObserver,org.apache.hadoop.hbase.coprocessor.transactional.TrxRegionEndpoint,org.apache.hadoop.hbase.coprocessor.AggregateImplementation"'
fi
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify hbase.coprocessor.region.classes through Ambari's configs.sh script."
    exit -1
fi
sleep 2
ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hbase-site hbase.hregion.impl org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion'
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify hbase.hregion.impl through Ambari's configs.sh script."
    exit -1
fi
sleep 2
ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hbase-site hbase.regionserver.region.split.policy org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy'
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify hbase.regionserver.region.split.policy through Ambari's configs.sh script."
    exit -1
fi
sleep 2 
ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh  -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hbase-site hbase.snapshot.enabled true'
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify hbase.snapshot.enabled through Ambari's configs.sh script."
    exit -1
fi
sleep 2
ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hbase-site hbase.snapshot.master.timeoutMillis 600000'
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify hbase.snapshot.master.timeoutMillis through Ambari's configs.sh script."
    exit -1
fi
sleep 2
ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hbase-site hbase.snapshot.region.timeout 600000'
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify hbase.snapshot.region.timeout through Ambari's configs.sh script."
    exit -1
fi
sleep 2
ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hdfs-site dfs.namenode.acls.enabled true'
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify dfs.namenode.acls.enabled through Ambari's configs.sh script."
    exit -1
fi
sleep 2
ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'hbase-site hbase.client.scanner.timeout.period 600000'
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify hbase.client.scanner.timeout.period through Ambari's configs.sh script."
    exit -1
fi
sleep 2
echo

#Change Zookeeeper settings

ssh -q -n $AMBARI_HOST "$AMBARI_DIR"'/configs.sh -u' "$ADMIN" '-p' "$PASSWORD" '-port' "$PORT" 'set' "$AMBARI_HOST" "$CLUSTER_NAME" 'zoo.cfg maxClientCnxns 0'
if [ $? != 0 ]; then
    echo "***ERROR: unable to modify maxClientCnxns through Ambari's configs.sh script."
    exit -1
fi
sleep 2
echo


#=====================================
# stop HBase to restart HDFS and pick up all the changes just made

poll_time=30
echo "***INFO: Restarting HBase to pick up config changes for Trafodion"
echo "***INFO: Stopping HBase..."
curl -k -u $ADMIN:$PASSWORD \
    -H "X-Requested-By: Trafodion" \
    -X PUT -d '{"ServiceInfo": { "state" : "INSTALLED" }}' \
    $URL/api/v1/clusters/$CLUSTER_NAME/services/HBASE > $TRAF_WORKDIR/traf_hbase_restart_temp

if [ $? != 0 ]; then 
   echo "***ERROR: Unable to stop HBase"
   echo "***ERROR: Please manually restart HBase through the Ambari web GUI"
fi 

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE $TRAF_WORKDIR/traf_hbase_restart_temp | grep Error | wc -l)

if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to stop HBase"
    echo "***ERROR: Please manually restart HBase through the Ambari web GUI"
fi

echo "***INFO: ...polling every $poll_time seconds until HBase stop is completed."
command_id=$(cat $TRAF_WORKDIR/traf_hbase_restart_temp | grep id | awk '{print $3}' | sed -e 's@,@@' )
echo "***DEBUG: Ambari command_id=$command_id"

# poll until stop is completed as a stop can take a while
completed=0
while [ $completed -eq 0 ]; do
    sleep $poll_time
    curl -k -u $ADMIN:$PASSWORD \
        $URL/api/v1/clusters/$CLUSTER_NAME/requests/$command_id \
        > $LOCAL_WORKDIR/hbase_restart_status_temp
    cat $LOCAL_WORKDIR/hbase_restart_status_temp
    echo "***INFO: ...polling every $poll_time seconds until HBase stop is completed."
    # if stop command is completed then completed will not equal 0
    completed=$(cat $LOCAL_WORKDIR/hbase_restart_status_temp | grep '"request_status" : "COMPLETED"' | wc -l)
    failed=$(cat $LOCAL_WORKDIR/hbase_restart_status_temp | grep '"request_status" : "FAILED"' | wc -l)
    if [[ "$failed" -ge "1" ]]; then
       echo "***ERROR: Unable to stop HBase"
       echo "***ERROR: Please manually restart HBase through the Ambari web GUI"
       exit -1
    fi
done

echo "***INFO: HBase stop completed"

#=====================================
#Stop Zookeeper

echo "***INFO: Stopping Zookeeper..."
curl -k --user $ADMIN:$PASSWORD \
    -H "X-Requested-By: Trafodion" \
    -X PUT -d '{"ServiceInfo": { "state" : "INSTALLED" }}' \
    $URL/api/v1/clusters/$CLUSTER_NAME/services/ZOOKEEPER > $TRAF_WORKDIR/traf_zoo_restart_temp

if [ $? != 0 ]; then
   echo "***ERROR: Unable to restart Zookeeper"
   echo "***ERROR: Please manually restart Zookeeper through the Ambari web GUI"
fi

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE $TRAF_WORKDIR/traf_zoo_restart_temp | grep Error | wc -l)

if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to restart Zookeeper"
    echo "***ERROR: Please manually restart Zookeeper through the Ambari web GUI"
fi

echo "***INFO: ...polling every $poll_time seconds until Zookeeper stop is completed."
command_id=$(cat $TRAF_WORKDIR/traf_zoo_restart_temp | grep id | awk '{print $3}' | sed -e 's@,@@' )
echo "***DEBUG: Ambari command_id=$command_id"

# poll until stop is completed as a stop can take a while
completed=0
while [ $completed -eq 0 ]; do
    sleep $poll_time
    curl -k --user $ADMIN:$PASSWORD \
        $URL/api/v1/clusters/$CLUSTER_NAME/requests/$command_id \
        > $LOCAL_WORKDIR/traf_zoo_restart_temp
    cat $LOCAL_WORKDIR/traf_zoo_restart_temp
    echo "***INFO: ...polling every $poll_time seconds until Zookeeper stop is completed."
    # if stop command is completed then completed will not equal 0
    completed=$(cat $LOCAL_WORKDIR/traf_zoo_restart_temp | grep '"request_status" : "COMPLETED"' | wc -l)
    failed=$(cat $LOCAL_WORKDIR/traf_zoo_restart_temp | grep '"request_status" : "FAILED"' | wc -l)
    if [[ "$failed" -ge "1" ]]; then
       echo "***ERROR: Unable to stop Zookeeper"
       echo "***ERROR: Please manually restart Zookeeper through the Ambari web GUI"
       exit -1
    fi

done

#=====================================
# restart HDFS to pick up all the changes just made

echo "***INFO: Restarting HDFS to pick up config changes for Trafodion"
echo "***INFO: Stopping HDFS..."
curl -k --user $ADMIN:$PASSWORD \
    -H "X-Requested-By: Trafodion" \
    -X PUT -d '{"ServiceInfo": { "state" : "INSTALLED" }}' \
    $URL/api/v1/clusters/$CLUSTER_NAME/services/HDFS > $TRAF_WORKDIR/traf_hdfs_restart_temp

if [ $? != 0 ]; then 
   echo "***ERROR: Unable to restart HDFS"
   echo "***ERROR: Please manually restart HDFS through the Ambari web GUI"
fi 

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE $TRAF_WORKDIR/traf_hdfs_restart_temp | grep Error | wc -l)

if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to restart HDFS"
    echo "***ERROR: Please manually restart HDFS through the Ambari web GUI"
fi

echo "***INFO: ...polling every $poll_time seconds until HDFS stop is completed."
command_id=$(cat $TRAF_WORKDIR/traf_hdfs_restart_temp | grep id | awk '{print $3}' | sed -e 's@,@@' )
echo "***DEBUG: Ambari command_id=$command_id"

# poll until stop is completed as a stop can take a while
completed=0
while [ $completed -eq 0 ]; do
    sleep $poll_time
    curl -k --user $ADMIN:$PASSWORD \
        $URL/api/v1/clusters/$CLUSTER_NAME/requests/$command_id \
        > $LOCAL_WORKDIR/traf_hdfs_restart_temp
    cat $LOCAL_WORKDIR/traf_hdfs_restart_temp
    echo "***INFO: ...polling every $poll_time seconds until HDFS stop is completed."
    # if stop command is completed then completed will not equal 0
    completed=$(cat $LOCAL_WORKDIR/traf_hdfs_restart_temp | grep '"request_status" : "COMPLETED"' | wc -l)
    failed=$(cat $LOCAL_WORKDIR/traf_hdfs_restart_temp | grep '"request_status" : "FAILED"' | wc -l)
     if [[ "$failed" -ge "1" ]]; then
       echo "***ERROR: Unable to stop HDFS"
       echo "***ERROR: Please manually restart HDFS through the Ambari web GUI"
       exit -1
    fi

done

echo "***INFO: Starting HDFS..."
curl -k --user $ADMIN:$PASSWORD \
    -H "X-Requested-By: Trafodion" \
    -X PUT -d '{"ServiceInfo": { "state" : "STARTED" }}' \
    $URL/api/v1/clusters/$CLUSTER_NAME/services/HDFS > $TRAF_WORKDIR/traf_hdfs_restart_temp

if [ $? != 0 ]; then
   echo "***ERROR: Unable to restart HDFS"
   echo "***ERROR: Please manually restart HDFS through the Ambari web GUI"
fi

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE $TRAF_WORKDIR/traf_hdfs_restart_temp | grep Error | wc -l)
if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to restart HDFS"
    echo "***ERROR: Please manually restart HDFS through the Ambari web GUI"
fi

echo "***INFO: ...polling every $poll_time seconds until HDFS start is completed."
command_id=$(cat $TRAF_WORKDIR/traf_hdfs_restart_temp | grep id | awk '{print $3}' | sed -e 's@,@@' )
echo "***DEBUG: Ambari command_id=$command_id"

# poll until start is completed as a start can take a while
completed=0
while [ $completed -eq 0 ]; do
    sleep $poll_time
    curl -k --user $ADMIN:$PASSWORD \
        $URL/api/v1/clusters/$CLUSTER_NAME/requests/$command_id \
        > $LOCAL_WORKDIR/traf_hdfs_restart_temp
    cat $LOCAL_WORKDIR/traf_hdfs_restart_temp
    echo "***INFO: ...polling every $poll_time seconds until HDFS start is completed."
    # if start command is completed then completed will not equal 0
    completed=$(cat $LOCAL_WORKDIR/traf_hdfs_restart_temp | grep '"request_status" : "COMPLETED"' | wc -l)
    failed=$(cat $LOCAL_WORKDIR/traf_hdfs_restart_temp | grep '"request_status" : "FAILED"' | wc -l)
    if [[ "$failed" -ge "1" ]]; then
       echo "***ERROR: Unable to start HDFS"
       echo "***ERROR: Please manually restart HDFS through the Ambari web GUI"
       exit -1
    fi

done

echo "***INFO: HDFS restart completed"

# wait to make sure HDFS is fully restarted and out of safemode
echo "***INFO: waiting for HDFS to exit safemode"
sudo su hdfs --command "hdfs dfsadmin -safemode wait"

#=====================================
# Start Zookeeper to pick up all the changes just made

echo "***INFO: Starting Zookeeper..."
curl -k --user $ADMIN:$PASSWORD \
    -H "X-Requested-By: Trafodion" \
    -X PUT -d '{"ServiceInfo": { "state" : "STARTED" }}' \
    $URL/api/v1/clusters/$CLUSTER_NAME/services/ZOOKEEPER > $TRAF_WORKDIR/traf_zoo_restart_temp

if [ $? != 0 ]; then
   echo "***ERROR: Unable to restart Zookeeper"
   echo "***ERROR: Please manually restart Zookeeper through the Ambari web GUI"
fi

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE $TRAF_WORKDIR/traf_zoo_restart_temp | grep Error | wc -l)
if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to restart Zookeeper"
    echo "***ERROR: Please manually restart Zookeeper through the Ambari web GUI"
fi

echo "***INFO: ...polling every $poll_time seconds until Zookeeper start is completed."
command_id=$(cat $TRAF_WORKDIR/traf_zoo_restart_temp | grep id | awk '{print $3}' | sed -e 's@,@@' )
echo "***DEBUG: Ambari command_id=$command_id"

# poll until start is completed as a start can take a while
completed=0
while [ $completed -eq 0 ]; do
    sleep $poll_time
    curl -k --user $ADMIN:$PASSWORD \
        $URL/api/v1/clusters/$CLUSTER_NAME/requests/$command_id \
        > $LOCAL_WORKDIR/traf_zoo_restart_temp
    cat $LOCAL_WORKDIR/traf_zoo_restart_temp
    echo "***INFO: ...polling every $poll_time seconds until Zookeeper start is completed."
    # if start command is completed then completed will not equal 0
    completed=$(cat $LOCAL_WORKDIR/traf_zoo_restart_temp | grep '"request_status" : "COMPLETED"' | wc -l)
    failed=$(cat $LOCAL_WORKDIR/traf_zoo_restart_temp | grep '"request_status" : "FAILED"' | wc -l)
     if [[ "$failed" -ge "1" ]]; then
       echo "***ERROR: Unable to start Zookeeper"
       echo "***ERROR: Please manually restart Zookeeper through the Ambari web GUI"
       exit -1
    fi

done

echo "***INFO: Zookeeper start completed"

#=====================================
# restart HBase to pick up all the changes just made

echo "***INFO: Restarting HBase to pick up config changes for Trafodion"
echo "***INFO: Starting HBase..."
curl -k -u $ADMIN:$PASSWORD \
    -H "X-Requested-By: Trafodion" \
    -X PUT -d '{"ServiceInfo": { "state" : "STARTED" }}' \
    $URL/api/v1/clusters/$CLUSTER_NAME/services/HBASE > $TRAF_WORKDIR/traf_hbase_restart_temp

if [ $? != 0 ]; then
   echo "***ERROR: Unable to restart HBase"
   echo "***ERROR: Please manually restart HBase through the Ambari web GUI"
fi

# in most cases curl does not return an error
# so curl's actual output needs to be checked, too
curl_error=$(grep TITLE $TRAF_WORKDIR/traf_hbase_restart_temp | grep Error | wc -l)
if [ $curl_error -ne 0 ]; then
    echo "***ERROR: Unable to restart HBase"
    echo "***ERROR: Please manually restart HBase through the Ambari web GUI"
fi

echo "***INFO: ...polling every $poll_time seconds until HBase start is completed."
command_id=$(cat $TRAF_WORKDIR/traf_hbase_restart_temp | grep id | awk '{print $3}' | sed -e 's@,@@' )
echo "***DEBUG: Ambari command_id=$command_id"

# poll until start is completed as a start can take a while
completed=0
while [ $completed -eq 0 ]; do
    sleep $poll_time
    curl -k -u $ADMIN:$PASSWORD \
        $URL/api/v1/clusters/$CLUSTER_NAME/requests/$command_id \
        > $LOCAL_WORKDIR/hbase_restart_status_temp
    cat $LOCAL_WORKDIR/hbase_restart_status_temp
    echo "***INFO: ...polling every $poll_time seconds until HBase start is completed."
    # if start command is completed then completed will not equal 0
    completed=$(cat $LOCAL_WORKDIR/hbase_restart_status_temp | grep '"request_status" : "COMPLETED"' | wc -l)
    failed=$(cat $LOCAL_WORKDIR/hbase_restart_status_temp | grep '"request_status" : "FAILED"' | wc -l)
     if [[ "$failed" -ge "1" ]]; then
       echo "***ERROR: Unable to start HBase"
       echo "***ERROR: Please manually restart HBase through the Ambari web GUI"
       exit -1
    fi

done

echo "***INFO: HBase restart completed"

#=====================================
# NOTE: These command must be done AFTER acls are 
#       enabled and HDFS has been restarted
echo "***INFO: Setting HDFS ACLs for snapshot scan support"
ssh -q -n $HDFS_NODE 'sudo su hdfs --command "hdfs dfs -mkdir -p /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -mkdir -p /apps/hbase/data/archive) command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su hdfs --command "hdfs dfs -chown hbase:hdfs /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -chown hbase:hdfs /apps/hbase/data/archive) command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su hdfs --command "hdfs dfs -setfacl -R -m user:'"$TRAF_USER"':rwx /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m mask::rwx /apps/hbase/data/archive) command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su hdfs --command "hdfs dfs -setfacl -R -m default:user:'"$TRAF_USER"':rwx /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m mask::rwx /apps/hbase/data/archive) command failed"
   exit -1
fi
ssh -q -n $HDFS_NODE 'sudo su hdfs --command "hdfs dfs -setfacl -R -m mask::rwx /apps/hbase/data/archive"'
if [ $? != 0 ]; then
   echo "***ERROR: (hdfs dfs -setfacl -R -m mask::rwx /apps/hbase/data/archive) command failed"
   exit -1
fi

# clean up files generated by Ambari's config.sh script
ssh -q -n $AMBARI_HOST 'rm $HOME/doSet_version*'

MODS_COMPLETE="Y"
sudo chmod 777 $TRAF_CONFIG
sed -i '/MODS_COMPLETE\=/d' $TRAF_CONFIG
echo "export MODS_COMPLETE=\"$MODS_COMPLETE\"" >> $TRAF_CONFIG
sudo chmod 777 $TRAF_CONFIG
source $TRAF_CONFIG

TRAF_CONFIG="trafodion_config"
TRAF_CONFIG_DIR="/etc/trafodion"

if [ $node_count -ne 1 ]; then
   cp $TRAF_CONFIG $LOCAL_WORKDIR
   $TRAF_PDCP $LOCAL_WORKDIR/$TRAF_CONFIG_FILE $HOME
   $TRAF_PDSH sudo mkdir -p $TRAF_CONFIG_DIR
   $TRAF_PDSH sudo cp -rf $HOME/$TRAF_CONFIG_FILE $TRAF_CONFIG_DIR
   $TRAF_PDSH sudo chmod 777 $TRAF_CONFIG
fi

